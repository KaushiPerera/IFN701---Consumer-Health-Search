\documentclass[]{article}
\usepackage{graphicx}

%opening
\title{Annotated Bibliography}
\author{Kaushi Perera}

\begin{document}

\maketitle


\section{Kogan, S., Zeng, Q., Ash, N., \& Greenes, R. A. (2001). Problems and challenges in patient information retrieval: a descriptive study. In Proceedings of the AMIA Symposium (p. 329). American Medical Informatics Association.}

\textbf{Introduction}

The main purpose of this study has been to ‘identify and classify the problems a patient encounters while performing information retrieval tasks on the Web, and the challenges it poses to informatics research’
The study which has been done by the researchers has consisted of ‘three types of IR tasks which has been performed on two Web sites’

The two websites involved are:

\textbf{1.	MEDLINEplus:} for Consumer health content

\textbf{Content of the website:} ‘information about specific diseases and conditions, and links to consumer health information from the National Institutes of Health, dictionaries, lists of hospitals and physicians, health information in Spanish and other languages, and information on clinical trials’

\textbf{2.	BWH's Web site: to find a physician}

How can a user search for a physician: ‘allows the user to search for a doctor by entering information in free-text format’. The searching criteria are ‘last name, first name, clinical interests, language and board certification’

\textbf{Method}

\textbf{Patient recruitment:} ‘Has been done in a patient family learning center of a large academic center’

Researchers have recruited 11 patients for this study

There have been mainly three parts for this study:

1.	‘Free-form search using MEDLINEplus’
2.	 ‘Freeform search using the Find-A-Doctor page on the BWH Web site’
3.	‘Scenario-based search using the Find-A-Doctor page on the BWH Web site’

For the first two parts patients have been allowed to ‘search for any health-related information of interest on the two Web sites’ 

For the third part ‘patients have been presented with pre-defined scenarios, and asked to find the relevant information’

There have been three measurement levels

\textbf{Effectiveness:} ‘This has been measured in terms of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) ratios for the search results’. The ‘queries have been reproduced and analysed after each participant completes their tasks’

\textbf{True Positive:} ‘Returns content that the subject wanted’

\textbf{True Negative:} ‘Returns no content, and content does not exist’

\textbf{False Positive:} ‘Returns some content, but not what the subject wanted’

\textbf{False Negative:} ‘Returns no content even though the content exists’

\textbf{Satisfaction and usefulness:} The participants have rated the satisfaction and usefulness during the interview which has been held after the tasks  

\textbf{Part I:} ‘In this task patients have searched any health-related information on the MEDLINEplus Web site which was of interest to them. Then the effectiveness of the retrieved results, participants’ satisfaction of the results and the usefulness of the results have been recorded’

\textbf{Part II:} ‘In this task patients have been asked to find a doctor on the BWH's Web site using the Find-A-Doctor page for any problems or concerns. Then as in part I, data regarding effectiveness, user satisfaction and usefulness of the results have been collected.’

\textbf{Part III:} ‘In this task Patients have been presented with pre-defined scenarios, and have asked to find a doctor using the BWH Find-A-Doctor page for each scenario’

\textbf{Results}

\textbf{Part I}

‘A total of 15 queries have been performed by the 11 patients, because some patients performed more than one query’

\textbf{(67%, 10/15):} of the query results have been found as not useful

\textbf{(73%, 8/11):} patients have not been satisfied with the results

In such cases the participants have been encouraged to use an alternative approach, such as health topics alphabet, or broad groups. In 64% of the cases, the alternative approaches have not improved effectiveness, satisfaction or usefulness.  

\textbf{(36%, 4/11):} of the patients have reported that the results have not been useful to them  

\textbf{Overall effectiveness:} Has been found to be quite low

\textbf{True Positive:} 20% matches

\textbf{False Positive:} 73% matches

\textbf{False Negative:} 7% matches

\textbf{Overall satisfaction:} 33% satisfied

\textbf{Overall perceived usefulness:} 33%

\textbf{Part II}

\textbf{Overall effectiveness:} Low

\textbf{True Positive:} 27% matches

\textbf{False Negative:} 73% matches

\textbf{Overall satisfaction:} 36% satisfied and 64% unsatisfied

\textbf{Overall perceived usefulness:} 36% satisfied and 64% unsatisfied


\textbf{Part III}

‘The summary table of the effectiveness (TP, TN, FN, FP), satisfaction, and usefulness of the results’

\includegraphics {Capture 1.png}

‘When considering the effectiveness, patients have yielded a true positive (TP) result 11% of the time. Satisfaction and usefulness of the results have also been quite low’

There have been three steps for the patient IR system

1. ‘Formulate query terms based on information needs’ 
2. ‘Obtain results from search engines by submitting query terms’
3. ‘Use query results to satisfy information needs’

‘The problems which have been identified as encountered by patients during IR cycle’

\includegraphics {Capture 2.png}

\textbf{Discussion}

‘It is identified that people have difficulty in getting access to the information they seek. This study has shown overall low rates of effectiveness, satisfaction, and usefulness’

‘Patient information retrieval affects all interactions using the Web - access to information, services and health care (telemedical programs, monitoring protocols, etc)’

It has also shown that ‘the three measures reflect different aspects of IR performance - effectiveness has not been correlated with satisfaction and usefulness’

It has also been identified that in order to solve the above mentioned problems in IR, ‘various layers of support need to be developed specifically for patients, such as including lexical tools (e.g., to correct spelling), semantic tools (e.g., to map patient terms to medical concepts), domain knowledge support (e.g., to bridge the gap between mental models), and filtering tools (e.g., to control information overload).’

‘This study has chosen one Web site for consumer health content (MEDLINEplus) and one for accessing services (Brigham and Women's Hospital Web site). It has been assumed that the content of these Web sites is representative of the health information needs of patients. However, it is identified that  generalizability of the results would require further study’

\section{Hersh, W., Price, S., & Donohoe, L. (2000). Assessing thesaurus-based query expansion using the UMLS Metathesaurus. In Proceedings of the AMIA Symposium (p. 344). American Medical Informatics Association.}

\textbf{Introduction}

\textbf{The aim of the study:} ‘To test whether query expansions using thesaurus relationships in the UMLS Metathesaurus could improve searching performance.’ 

The relationships of Thesauri: ‘synonym, hierarchical, and related’

\textbf{1.	Synonym:} ‘Denotes equivalence. This means different terms representing the same underlying concept, such as cancer vs. carcinoma’

\textbf{2.	Hierarchical:} ‘Indicates a broader/narrower classification. This means that the child term has an is-a relationship with the parent. For instance, Angiotensin Converting Enzyme Inhibitors vs. Captopril’

\textbf{3.	Related:} ‘Indicates some other type relationship that has been deemed important. For instance, hypertension vs. antihypertensive agents’


‘Another part of most thesauri is a concept definition, and this study has also examined this component for query expansion.’

‘The experiments have been done using OHSUMED test collection. This has been based on 106 queries and nearly 350,000 MEDLINE references from clinical journals over the 1987-1991 time period’

‘The performance has been measured using the two measurements recall (the proportion of relevant documents in the collection retrieved by a query) and precision (the proportion of relevant documents returned by the query)’

'11-point average has been used as a composite performance measure. This measure gives precision at fixed points of recall (<10%, 10%, 20%, etc. up to 100%) from the ranked list.’

\textbf{Methods}

\textbf{Aim:} ‘To assess whether expanding queries using terms from thesaurus relationships and definitions could enhance retrieval performance‘ 

‘The experiments have been done using the OHSUMED test collection and the SMART retrieval system version 11.0’

‘The standard SMART recall-precision routines have been used to measure performance’

‘Researchers have developed a set of manually selected Metathesaurus concepts for each query, so as to assess expansion of terms using Metathesaurus relationships’

‘Queries have been submitted to the SAPHIRE system for suggested Metathesaurus terms. Then the SAPHIRE system breaks out phrases which are in between stop words before sending them to the concept-matching algorithm’

‘Queries with parsimonious (i.e., complete but non-overlapping) coverage of all medical concepts by Metathesaurus terms have been left as is’

‘Queries with incomplete or multiple matches have been manipulated manually to obtain as close a set as possible of parsimonious concepts. As a result, the 106 queries have yielded 298 terms’

\textbf{Run 1:} ‘This is the baseline established with the OHSUMED queries using the previously established best weighting scheme’ 

\textbf{Run 2:} ‘This run has consisted of the same run using the manually assigned Metathesaurus terms assigned’ 

\textbf{Run 3:} ‘In this run synonym has been assessed by adding all possible synonym variants of a term to the query. This has been done by adding all words that occurred in the Metathesaurus word file (MRXW) for each Metathesaurus concept of each term’ 

\textbf{}Run 4:} ‘In this run hierarchical relationships have been evaluated by following the links of parent and child terms in the Metathesaurus MRREL file. Therefore, terms from any vocabulary from the concept have been added to the query. Hence, the expansion by adding one level of children terms has been done for Run 4’

\textbf{Run 5:} ‘In this run the expansion has been done all the way to the "bottom" of the children hierarchy’ 

\textbf{Run 6:} ‘In this run one level of expansions have been done for the parent terms’

\textbf{Run 7:} ‘In this run all-level of expansions have been done for the parent terms’’

\textbf{Run 8:} ‘In this run related terms have been assessed by adding terms designated as related by the RO relationship in the MRREL file (Even though the researchers have assessed the RL relationship in this file, another related term designator in MRREL, only 2 of the 298 terms could have been expanded)’

\textbf{Run 9:} ‘In this run it has been assessed whether text from the term definition could be used to expand queries effectively. For every concept that had a definition, its text has been added to the text of the query’

‘For each of these runs, a recall-precision table has been obtained that yielded precision at fixed points of recall. The performance measure for each query has been the 11-point average precision at each point of recall’

‘Recall and precision also have been measured for 30 documents. This is a standard approach in IR evaluation which gives a point value of recall and precision for a number of documents that users are willing to look at after a search’

‘Therefore, the performance measures for each run have been the 11-point average precision, recall at 30 documents, and precision at 30 documents for each query in the run’ 

‘Repeated measures analysis of variance has been used to assess statistical significance, with Scheffe tests used to compare statistical significance between baseline and subsequent runs for the three measures’

\textbf{Results}

‘The manually designated Metathesaurus terms (run 2) have performed comparably to the baseline. However, every other expansion have degraded aggregate performance. All of the differences have been identified as statistically significant with the exception of word expansions’

‘All hierarchical expansions have been identified as worsening the performance’

‘All-level expansions have been identified performing poorer than single-level’

‘However, each expansion has resulted in some queries with improved performance’

‘Word expansion has been able to provide the highest number of improved queries, with over 40% of queries showing improved recall-precision’

‘Both one-level and all-level children expansions as well as the one-level parent and related term expansions have shown benefit about one-quarter of the time’

\textbf{Discussion}

‘It has been identified that expanding queries with Metathesaurus terms do not confer any aggregate advantage as measured by recall and precision’

‘On the average it has been understood that adding synonymous, hierarchical, or related terms or term definitions, are not able to improve the retrieval performance’

‘Some instances, such as word expansion, has been able to improve the performance over the baseline in 40% of queries. There it has been assumed that there may be a role for expansion when applied in certain instances’ 

\textbf{Limitations}

1.	This study has only focused on ‘one test collection which has only one type of document, the MEDLINE reference. However, having other test collections, particularly those derived from full-text databases, may have a better chance of accruing performance benefits than bibliographic databases such as MEDLINE. For instance, it has been shown that the presence of MeSH terms leads to a 10% improvement in performance as measured by average precision’ 

2.	‘This study has assessed only one type of IR system, the word-statistical system. However, with a different system, such as a Boolean system it has been possible to yield different results than what has been achieved in this study’

3.	‘The batch nature of the experiments. Researchers have attempted to systematically assess benefit from different expansion techniques on the collection as a whole. However, there has been a possibility that searchers could add terms in an individualized manner leading to better performance’

\textbf{Conclusion:}

According to this study, ‘thesaurus-based automated query expansion does not necessarily improve searching performance’. This is ‘in contrast to automated query expansion based on words and indexing terms from the document’.

‘The use of thesauri tools such as the UMLS Metathesaurus can provide benefit for IR tasks such as automated indexing and query expansion in a substantial minority of queries but not in the aggregate’
 
‘Further improvements in thesauri, indexing approaches, and other language technologies are important, to be done with more systematic study of users and their information needs’

\section{McCray, A. T., Ide, N. C., Loane, R. R., & Tse, T. (2004, September). Strategies for supporting consumer health information seeking. In Medinfo (pp. 1152-1156).}

\textbf{Introduction}

According to a previous study conducted, there are ‘three broad classes of query failures’

1.	‘Due primarily to content coverage’
2.	‘Due to user query formulation’
3.	‘Due to system functionality’

\textbf{The aim of the study:} ‘To develop a variety of techniques to assist user information seeking’
How to assess the efficacy: ‘Researchers have submitted a large number of the original failed queries to the new search engine to determine whether these techniques would result in better system performance’

\textbf{Materials and Methods}

‘For this study researchers have used queries that have been submitted as basic searches to ClinicalTrials.gov during the month of November 2001’

‘Then they have identified queries which resulted in at least one retrieval result, queries which have resulted in no retrieval and queries which have been offered with spelling assistance’

‘1000 of the failed queries have been analysed to determine the cause of the failure, for instance, no content available at the time’

‘It has been identified that many failures have been due to characteristics of the query, such as misspellings, abbreviatory expressions, run-together phrases, and incorrect use of search operators’ 

‘The researchers had a commercial search engine that had been enhanced by their own terminology server. This terminology server handled lexical variation, including providing spelling help, and it has allowed for expanding queries with synonyms whenever these have been available. Then the researchers have developed an in-house XML-based search engine (SE), which works together with the terminology server and a variety of their own retrieval algorithms. Currently researchers use SE in the production ClinicalTrials.gov system as well as in several other public systems’

\textbf{Tasks of SE search and retrieval system:}

1.	‘It indexes and searches structured documents in eXtensible Markup Language (XML) format’

2.	‘It first parses these documents according to a fine-grained tokenization method, with punctuation characters treated as separate tokens. For instance, the string “non-hodgkin’s lymphoma” will be treated as 6 tokens which are “non, -, hodgkin, ‘, s, lymphoma”.’

3.	‘The index process generates a dictionary of all the words in the XML document set each time new data come into the system, and the synonymy set is pruned to the set that overlaps the words in the corpus’

4.	‘At retrieval time this means that any query suggestions are limited by a “closed world assumption”. For instance, if the user submits the misspelled query “osteoparosis”, the system will suggest both “osteoporosis” and “osteopetrosis”, but only if there are records for both of these conditions in the current document set’

\textbf{Terminology Server:} ‘Provides synonymy and lexical information from the Unified Medical Language System. The stopword list consists of a small number of words, such as conjunctions and prepositions. Stopwords are retained in the query text, but are treated specially during query analysis’

\textbf{Tags within the XML documents:} ‘Define names for various parts of the document. SE has used a mapping file to associate a named search area with a set of document parts. Each document part has been given a weight indicating its relative importance within the named search area. For example, in ClinicalTrials.gov a hit within the brief title document part (with a weighting of .95) has considered to be more relevant than a hit in the location document part (with a weighting of .55)’

\textbf{Searching for documents:}

‘The user have been asked to submit a query to the Web application. The query either retrieves a result set directly, or if nothing is found, the system offers suggestions that may help the user either reformulate the search or try one of the searches suggested by the system’

\textbf{Search Strategy:}

‘Generates multiple interpretations of the user’s query, merging the result sets for all interpretations. Interpretations have been generated by breaking the text into all possible phrase combinations. The most restrictive interpretation has treated the entire text as a single phrase. The least restrictive interpretation has treated the text as the conjunction of all the meaningful words. The resulting expression has been a “relaxed” alternative to treating the entire text as a phrase’

‘All search terms have been expanded with plurals, possessives, hyphen variants, compound words, and synonymy, if possible. All possible relaxed alternatives have been evaluated, weighted, and combined to produce the final result’

\textbf{Steps to evaluate the relaxation search:} 

1.	‘Performing phrase searches, which produces a list of hits for each piece and its expansions’

2.	‘Merging the hits, which produces a set of scored documents for each piece’

3.	‘Applying AND operators to the pieces, which produces a set of scored documents for each relaxed alternative’

4.	Weighting the alternatives so that the more relaxed alternatives contribute less to the overall scores. A penalty is applied for each inserted AND operator

5.	‘Applying an OR operator to the weighted alternatives, which produces a set of scored documents for the full relaxation search’

‘A document containing all the words of the query text, adjacent and in the same order as expressed in the query, has been received a higher score than a document containing all the words somewhere, but not adjacent to each other’

‘Pieces of a query that are known terms have been expanded with synonyms and find relevant documents, and also have contributed to higher document scores. Pieces of a query that are not recognizable have considered as trivial expansions, rarely occur in the data, and do not contribute significantly to the document scoring’

Search strategy algorithm example for the query ‘heart attack in elderly’

\includegraphics{Capture 3.png}

The steps which are followed by the algorithm:

1)	Break the query text into words and identify stopwords.

heart = meaningful word 1 
attack = meaningful word 2
in = stopword
elderly = meaningful word 3

2)	Create a list of all possible relaxed alternatives by inserting one or more AND operators

( heart attack in elderly )
( heart ) AND ( attack in elderly )
( heart attack ) AND ( elderly )
( heart ) AND ( attack ) AND ( elderly )

3)	Extract the pieces from the relaxed alternatives

heart
attack
elderly
heart attack
attack in elderly
heart attack in elderly

4)	Expand each piece with its synonyms and inflectional variants.

heart → hearts, cardio
attack → attacks
elderly → older adult, older adults, geriatric, geriatrics,
aged person, aged persons
heart attack → heart attacks, heart infarction,
heart infarctions, myocardial infarction,
myocardial infarctions, …
attack in elderly → attacks in elderly
heart attack in elderly → heart attacks in elderly

5)	Weight the alternatives (synonyms and inflectional variants are excluded for expositional clarity)

1.00 * ( heart attack in elderly ) OR
0.10 * ( ( heart ) AND ( attack in elderly ) ) OR
0.10 * ( ( heart attack ) AND ( elderly ) ) OR
0.01 * ( ( heart ) AND ( attack ) AND ( elderly ) )

‘The relaxation approach has produced a set of intermediate results that can serve as query suggestions’

‘The response time for each search is important and the search results have been expected to be provided within a reasonable time’

‘According to the experience it has been said that if the phrase search ‘heart attack in elderly’ can be performed in 50 ms, a query with seven meaningful words can be fully evaluated with suggestions in less than a second’

‘If the result set is not empty, then the results of the search have been displayed together with a link to the “query details” page. If the result set is empty, then the query details page has been immediately displayed’

‘The researchers have decided to test the performance of SE in order to compare it’s performance their earlier search methods. Therefore, the queries which have been posted to ClinicalTrials.gov have been re-evaluated. They have run all 155,777 queries to compare the retrieval results with the previous retrieval results which have been obtained using earlier search methods’

‘Also another set of 1000 queries which had resulted in query failures, have been run through SE to determine whether the new system performs any better’

‘In order to control the content, the researchers have used the same 2001 ClinicalTrials.gov data set, rather than using the current content. This has been done in order to emphasise the efficacy of SE without considering the addition of clinical trial content as a search engine improvement’

\textbf{Results}

\textbf{Query details button:} By clicking this button ‘the user has been able to see the precise search executed by the system, including how the query was parsed by the system and any synonyms that has been searched’

\textbf{Query failure:} ‘the system offers immediate assistance to the user in the form of query suggestions’

\textbf{Query details page:} ‘This page has allowed users to modify their queries and to receive feedback. This page has contained suggested query variants and a table showing terms and words from the user's query and how often they have appeared in the document set. The query suggestions have been displayed with an explicit syntax that includes Boolean operators’. The aim of this page has been to ‘guide the user in the differences between similar queries’

\textbf{TryIt! link:} Users have been able to ‘execute a suggestion through TryIt! link’

\textbf{Individual terms report:} ‘This has listed recognized terms and the number of times they have been found in the data set’. ‘Also this has listed the compound terms first followed by the individual words’. ‘If a term or its synonyms have not been found in the data set, then this has been marked as “none”’. ‘Available synonyms for each term and word has been shown under “Also searched’’ section’

\includegraphics{Capture.png}

\includegraphics{Capture 5.png}   

The results of running queries in the old system and the new system

\includegraphics{Capture 6.png}   

‘In the 2001 system, 20% of the queries have been failed completely, while in the current system a somewhat smaller percentage (16%) has failed’

‘A decrease could have been seen in the number of records in the 2003 system. Since query suggestions are an important feature of SE, researchers have reported cases where the system is able to offer suggestions. The terminology server feature of the original 2001 system has also included a suggestion capability, but these have been exclusively spelling suggestions. Thus, all “suggestions offered” for the 2001 system have been examples of spelling correction, while the suggestions offered for the 2003 system has included not only spelling, but also other suggestions as shown in the individual terms report’

Results of running 1000 failed queries

\includegraphics{Capture 7.png} 

Because these queries have been failed by definition, in 2001, zero records have been found

‘The current system has cut the earlier failure rate almost in half (411/844), in most cases providing a suggestion (568) rather than directly finding records (21)’

‘The 2001 system has been able to make a suggestion in only 16% of the cases. The 2003 SE system, on the other hand, has been able to do so for 57% of the queries’

\textbf{Discussion}

‘The 2003 system has a smaller number of failed queries than the earlier system. The difference is a modest 4%, but still it reflects some important underlying changes to the search methodology, including “query relaxation”, improved tokenization, and synonym enhancement’

‘For example, the query "mini-transplants" previously has not been able to match the document term "mini-transplant" because there has been no known variant for that term. SE now tokenizes the term into "mini, -, transplants," generates the variant "transplant," recombines the tokens ("mini-transplant"), and retrieves five documents’

‘Also, some of the previously failed queries have been able to provide results using SE because additional synonyms have been added to the terminology server either by the UMLS, or through synonym enhancement activities. For instance, the tradename "Xeloda" has been expanded to "capecitabine".’

‘In addition, query relaxation has been able to interact with phrase level synonymy. For example, the earlier failed query “multiple system atrophy with postural hypotension” has been able to retrieve a document because the phrase “postural hypotension” finds the synonym “orthostatic hypotension”’

‘The 2001 system has found 7% more records than the 2003 system. This has been because of the fact that the 2001 search algorithm allowed, as its least restrictive search, any word in the query to retrieve a document. That is, if nothing has been otherwise found, the query has been reduced to an OR expression of its constituent words and a retrieval set has been presented to the user.’

‘In the current system, the search has not been executed until the user interacts with the system. This query suggestion capability has been identified as the largest overall improvement. The earlier system has provided only spelling suggestions, while the current system has been able to provide suggestions based on query relaxation methods. For example, the query “high dosage of chemotherapy on breast cancer cells” which has failed in the earlier system, now offers as one of its suggestions “dosage AND chemotherapy AND breast cancer”, which has been able to retrieve one document. Similarly, the failed query “nutrition oncology cancer diet”, now suggests “nutrition AND oncology”, which has been able to retrieve two documents’ 

‘Because not all query suggestions will reflect what the user intended, providing feedback to users on what is available allows them to either reformulate or abandon their search’

‘Given the difficulty and ambiguity of query interpretation, this system has attempted to convey to users how their queries have been interpreted and how they relate to the underlying document set’

\textbf{Conclusion}

‘The efficacy of the search techniques has been tested by submitting a large number of user queries to the newly developed SE search engine. The system performance has been measured according to the documents retrieved when each user query is executed’

‘Overall, the number of query failures is shown to be reduced, but the largest improvement has been found in the system’s query suggestion capability’

‘For a subset of query failures, the new system has been able to cut the earlier failure rate almost in half, in most cases providing a suggestion rather than directly finding records’

‘The techniques described here have provided a new approach for responding to user queries. The techniques have found to be tolerant of certain types of errors and, importantly,  have provided feedback to help users in reformulating their queries’

‘It is said that the investigation is being continued regarding the feedback that is given to the user with the goal of improving the assistance the system offer to users as they seek information on consumer health information systems’  

\section{Billerbeck, B., Scholer, F., Williams, H. E., & Zobel, J. (2003, November). Query expansion using associated queries. In Proceedings of the twelfth international conference on Information and knowledge management (pp. 2-9). ACM.}

\textbf{Introduction}

\textbf{Query Expansion:} The additional query terms are extracted from highly ranked documents, on the assumption that these are likely to be relevant

‘The general approach considered in this study is that the source of expansion terms need not be the collection itself, but could be any document set whose topic coverage is similar to that of the collection, and may thus suggest additional query terms’

‘It has been investigated whether query associations can be used for query expansion.’

‘Given a log containing a large number of queries, it is straightforward to build a surrogate for each document in a collection, consisting of the queries that have been a close match to that document’

‘According to earlier studies query associations has the ability to provide a useful document summary; that is, the queries that match a document are a fair description of its content’

‘In this study the investigation has been aimed at identifying whether query associations can play a role in query expansion’

Ranking with query expansion consists of three phases: 

1.	‘ranking the original query, against the collection or a document set’

2.	‘extracting additional query terms from the highly ranked items’

3.	‘then ranking the new query against the collection’

‘It has been shown that query associations are a highly effective source of expansion terms’

‘It is also said that on the TREC-10 data, average precision rises from 0.158 for optimised full text expansion to 0.189 for expansion via association, a dramatic relative improvement of 19.5%’ 

\textbf{Background}

\textbf{Query Expansion}

Relevance feedback has been one of the oldest techniques used for improving the effectiveness of information retrieval

Query expansion done by adding related terms extracted from like the top 10 results retrieved from the original query and using this expanded query to retrieve results, have shown that this method has the ability to improve effectiveness

‘It is understood that query expansion is not always effective, because query expansion using local analysis with fixed parameters is not robust’. For instance, ‘in some data collections, such as TREC web data, expansion does not appear to work effectively’

\textbf{Query expansion}

‘Past queries can be used for improving automatic query expansion’. ‘The results of past queries have been used to form an affinity pools, from which expansion terms are then selected’. ‘Candidate expansion terms have been identified by running the original query against this pool. Individual terms have been then selected from the top-ranked documents using a TF-IDF term-scoring algorithm’. ‘Previous work has shown that this technique has the ability to improve relative average precision’

\textbf{Query association}

‘User queries become associated with a document if they share a high statistical similarity with the document’

‘There has been a maximum set of queries that can be associated with one document’. ‘The least similar associated query have been dynamically replaced with a new, more similar query’ 

‘For instance, when query 1 is issued this query has been associated with the top 5 results of this query. Then the second query, query 2 is issued and it has been associated with its top 5 results. If both queries have one result in common, this common document has two associated queries. Then the third query is executed and if the top 5 results of the third query also has that previously mentioned document as one of its results, then this document now has three associated queries. If the similarity scores between queries and the common document are ordered such that q1 < q2 < q3, then q1 has been replaced with the query q3 as an association for that document’ 

‘Query association has been proposed for the creation of document summaries, to aid users in judging the relevance of answers returned by a search system’

‘It is identified that the better way to have query associations is that having small summaries composed of high-quality associations’

\textbf{Anchor Text}

‘This simply means that having document surrogates which are created from anchor text for finding entry pages to web sites’. ‘According to previous experiments document surrogates derived from anchor text have been significantly more effective than full-text retrieval for a page-finding task’. ‘But anchor text have not been useful for topic finding tasks’

\textbf{Generalised Expansion}

The process followed in this study is as follows

1.	‘A query has been submitted to the search system, and the top R answer documents are obtained, based on a particular collection’

2.	‘Then a set of candidate expansion terms have been identified based on the top R documents, or surrogates corresponding to these documents’

3.	‘Then the top E expansion terms have been selected, using a term selection value formula (In this case Robertson and Walker’s term selection value formula)’

4.	‘Finally, selected terms have been appended to the original query, which is then run against the target text collection’

‘If the researchers use the same collection of documents for all steps, then query expansion is of the standard form’ 

‘Surrogate documents constructed from query associations have also been used for query expansion. In this approach, queries have been associated with documents, then the set of associated queries for a document is used to represent the document. These associated queries could have been incorporated into the expansion framework in either the first step (ranking), the second step (term selection), or both’

‘The three options are as follows’

1.	‘Running the original query on the full text collection, after which the top E expansion terms are selected from the set of queries that have previously become associated with the top R documents returned from running the original query. This scheme has been called full-assoc, as step one of expansion is based on the full text of documents in the collection, step two is based on query associations’

2.	‘Initially rank directly on the surrogates built from associations, then choose expansion terms from the original documents. This scheme has been called assoc-full’

3.	‘Rank on the document surrogates built from associations, then select the E expansion terms from the top R ranked surrogates. This scheme has been called assoc-assoc, as associations are used for both steps 1 and 2 of expansion’

Why the use of associations for expansion is attractive?

1.	‘It means that the additional terms have already been chosen by users as descriptors of topics’

2.	‘In contrast to expanding directly from queries, there is more evidence of relevance. For instance, a surrogate document constructed from associations has many more terms than an individual query. Also, the fact that the queries have associated with the document means that, in some sense, the terms have topical relationships with each other’
 
‘Assoc-assoc option has not rely directly on the document collection that is searched’

‘In the second two schemes assoc-full and assoc-assoc, rather than relying on ranking the documents in the collection to find relevant associations, the associations themselves have been treated as documents that are ranked and used as sources for expansion terms’

‘The “aboutness” of the individual documents has been captured and made use of’

‘As an alternative way, individual queries could have been treated as documents, then source expansion terms by initially ranking the individual queries, and selecting E terms from the top R past queries returned. This scheme has been called query-query’

‘Another one approach has been to use anchor text, where the E expansion terms from the top R anchor text surrogates has been chosen, and then search the surrogates again using the expanded query. This scheme has been called link-link’

‘In order to determine the parameters for each scheme, the TREC-9 queries and relevance judgements have been used to find good parameter settings’
 
\textbf{Experiments}

\textbf{Setup}
	
In this study the experiments have been conducted using ‘the TREC WT10g collection, a 10 gigabyte collection of data crawled from the World Wide Web in 1997’. ‘This set of documents have had a high level of interconnectivity, allowing link-based retrieval methods to be evaluated’

‘Fifty test queries and corresponding relevance judgements for this collection have been developed’

‘Usually TREC queries consist of four parts, such as a query number; a title field, a description of the user’s information need and a narrative‘.  For these experiments the researchers only have used ‘the title field for the initial query, as this is the most representative of a typical web information retrieval task’ 
‘BM25 ranking measure have been used to obtain the similarity scores’

‘The full text retrieval results that have been used as a baseline for these experiments, are from runs that use no query expansion’

‘The query associations have been built using two logs from the Excite search engine, each taken from a single day in 1997 and 1999’

‘917,455 queries have been chosen to associate with the collection’

‘The average number of associations per document after processing has been 5.4, and just under 25% of documents in the collection had zero associations’

‘The associations have been built as a batch job in these experiments’

‘Association parameters have been set as M=19 and N=39 which means that each document has a maximum of 19 associated queries and the top 39 documents returned in response to a query have been associated with that query’
	
‘These parameters have been determined by creating document surrogates from the associated queries based on different parameter combinations, and testing retrieval effectiveness by evaluating searches on these surrogates’ 

‘For the experiments where expansion terms have been chosen directly from queries (with no association), the researchers have also used the 917,455 filtered entries from the Excite 1997 and 1999 Excite logs’

‘The anchor text for the experiments have been obtained by identifying anchor tags within the WT10g collection, and collating the text of each anchor that points in to a particular document into a surrogate for that document’

\textbf{Significance}

‘The significance of the results has been evaluated using the Wilcoxon signed rank test. This is a non-parametric procedure used to test whether there is sufficient evidence that the median of two probability distributions differ in location. For information retrieval experiments, it could have been used to test whether two retrieval runs based, for example, on different query expansion techniques, differ significantly in performance’. ‘As this test has taken into account the magnitude and direction of the difference between paired samples, this test has been identified as more powerful than the sign test’. ‘Also because this is a non-parametric test, it has not been necessary to make any assumptions about the underlying probability distribution of the sampled population’

‘By using a statistical significance test, it has enabled conclusions to be drawn about whether a variation in retrieval technique leads to consistent performance gains’

\textbf{Results}

‘In these experiments, the researchers have compared a baseline full text retrieval run with the expansion variants’

‘According to the findings of the experiments the full-full scheme has been the conventional approach to query expansion’

‘The results have been obtained for two parameter settings’

1.	full-full (A): R = 10 and E = 25

2.	full-full (B): M=19 and N=39

‘The full-full schemes have not been able to offer significantly better results than no expansion, except in the R-P measure for the optimal parameter settings (the relative improvement has been identified as 9%)’

‘It has shown that the novel association-based schemes are effective for query expansion’

‘The assoc-assoc scheme has been 18%–20% better in all three measures than full-full (a) expansion (an absolute difference of 0.03–0.05) and 26%– 29% better than the baseline no-expansion case (an absolute difference of 0.04–0.07’

‘The assoc-full schemes have been even more effective in the stringent R-Precision measure’

‘All results have been significant at least at the 0.10 level’

\textbf{Conclusion:} ‘Query association has been an effective tool in the initial querying stage prior to expansion’

‘According to the quartiles and variance of different expansion methods, at the lower quartile, all the methods have degraded performance somewhat. However, the extent of degradation has shown to have a little relationship to average effectiveness’. ‘At the upper quartile, however, there have been clear differences between the methods’

‘Assoc-assoc scheme has illustrated its utility over the full-full approach’

‘For instance, for the query “earthquakes”, the average precision for the assoc-assoc scheme has been 0.1706, compared to 0.1341 for no expansion and 0.1162 for the full-full approach. This has been dentified as a direct result of the choice of terms for expansion’

‘For the assoc-assoc scheme, the expanded query has been large and has appeared to contain only useful terms’

‘However, for the full-full(b) scheme the query has been narrow’

‘It has also shown that query-query, full-assoc, and link-link schemes offer limited benefit for expansion’

‘Without association to documents, the queries have shown to be ineffective, because the median length of the queries has been two words, that is, the queries have very little content when not grouped together as associations’

‘The full-assoc scheme has also found to be ineffective for similar reasons’

‘Query associations have been an excellent source of expansion terms, but have been less effective as document surrogates in the final step of the retrieval process’

‘The link-link scheme has been shown to be significantly worse than no expansion for the Average Precision measure’. This has been due to the reason that ‘anchor text is shown to be of utility in home or named page finding tasks, while the queries used in these experiments are topic finding tasks’

‘When the experiments have been conducted on TREC disks 4 and 5 which consist of newswire and similar data, the experiments have been unsuccessful’. ‘The reason for this is identified as the query logs, drawn from the web, are inappropriate for this data: based on a small sample, it seems that many of the queries in the log do not have relevant documents’. ‘Therefore, the process of creating newswire associations from search-engine logs has been unlikely to be successful’

‘Therefore, it has been concluded that, query association based expansion is only of utility if queries are available that are appropriate for the collection being searched’ 

\textbf{Conclusions}

‘Query expansion has shown to be an effective technique for information retrieval’

‘However, parameter settings that work well for one set of queries may be ineffective on another’

‘In this study the researchers have investigated alternative techniques for obtaining query expansion terms, with the aim of identifying techniques that are robust for different query sets’

‘In this study the researchers have identified a successful expansion source for web retrieval’. ‘This source is known as query association, that is, past queries that have been stored as document surrogates for the documents that are statistically similar to the query’

‘According to the experiments conducted using prior query associations, researchers have found that expanding TREC-10 web track topic finding queries using query associations and then searching the full text is 26%–29% more effective than no expansion, and 18%–20% better than an optimised conventional expansion approach’

‘Also, the results of these experiments have been found to be significant under statistical tests’

‘Therefore, it has been concluded that query associations are a powerful new expansion technique for web retrieval’

\section{Plovnick, R. M., & Zeng, Q. T. (2004). Reformulation of consumer health queries with professional terminology: a pilot study. Journal of medical Internet research, 6(3).}

\textbf{Introduction}

It has been identified that most of the consumer health search queries do not retrieve information as to satisfy consumers’ health information need because of the lay terms used and the length (short) of the queries.

In this study the researchers have investigated whether there would be any different to the performance if the consumer health queries are reformulated with professional terminology. 

In order to investigate the ‘effect of query reformulation the researchers have utilized original consumer health-information queries with explicit information needs and have chosen the Unified Medical Language System (UMLS) Metathesaurus, as the terminology source’

‘The effect of reformulation has been studied in two different search spaces: the broad scope of a commercial search engine and the more limited scope of a single consumer health-information site’

\textbf{Methods}

\textbf{Collection of Consumer Queries and Search Goals}

‘Consumer queries and search goals have been collected by recruiting patients and visitors from public areas of Brigham and Women's Hospital (a large teaching hospital in Boston)’

‘Then while conducting an interview, the participants have been asked to describe their health-information needs to the interviewer’

‘Then each participant has been given the opportunity to search the Internet on a laptop computer to find the answer to his or her specific question or questions’

‘The free-text queries generated by participants for these searches have been recorded for further analysis’
‘Search goals have also recorded by the researchers based on interviews with the participants’

\textbf{Selecting Queries for Further Testing]

‘The UMLS Metathesaurus stores information about biomedical concepts compiled from numerous vocabularies and sources’

‘In this UMLS Metathesaurus  synonyms and inter-concept relationships are two of the many attributes recorded for each concept, with one term chosen as the preferred English name for each concept’

‘For this study, the search queries generated by consumers have been hand-mapped to Metathesaurus-preferred concept names. For instance, the consumer query "stroke" has been mapped to a synonym of the Metathesaurus concept "cerebrovascular accident’

‘Some consumer queries, have been found to be identical to the primary term used in the Metathesaurus. Therefore, this study has only chosen queries that are not equivalent to Metathesaurus preferred terms, so that they could be reformulated’

\textbf{Gold Standard Answer Generation}

‘For each consumer question, a gold standard answer specific to the consumers' information needs has been generated by an investigator with medical training. These Gold standard answers have been used to assess and compare the results generated from the Internet searches conducted for this study’

\textbf{Query Reformulation}

‘User queries have been mapped to concepts in the UMLS Metathesaurus’

‘Only queries that had at least one term found to be in the list of synonyms for a preferred concept name have been selected for reformulation’

‘Queries have been reformulated by replacing the user term with the preferred synonym’

‘Terms within queries that already corresponded to preferred concepts have been left unaltered. For instance, the word "thyroid" has been left unchanged in the reformulation of the user query "thyroid abs test"’

‘Also, this study has altered only one concept name at a time. For example, two reformulations have been generated from the user query "herbal treatment cancer" which are, "herbal therapeutic aspects cancer" and "herbal treatment malignant neoplasms”’

\textbf{Internet Search Using Original Consumer Queries and Reformulated Queries}

‘Both the consumers' original queries and the modified queries have been used to conduct Internet searches’

‘Quotation marks have been placed on each end of the query text (eg, the search query for flat head has been "flat head"’)

‘The Google search engine has been used to search both the consumer health-oriented site (MedlinePlus) and the general Internet (Google)’

‘Searches using the search engine included on the MedlinePlus site have yielded several lists of results organized by health topic, whereas Google-initiated searches have yielded a single list ordered by relevance’ 

‘The Google search engine has been utilized so that search results would be identically formatted and therefore more suited for comparison’

‘Also, every search has been limited to English language pages using Google's "Advanced Search" language feature’

‘Information, such as the date and the total number of results for each search have been recorded’

‘Then the first 30 hits of every search, the number of documents that users are reasonably willing to look at after a search have been assessed in order to identify the presence of the gold standard answer to the participant's original question’ 

‘A result page has been considered to contain the gold standard answer if:’

1.	‘the answer could be found by following no more than one link from the initial page’

2.	‘at least 90% of the established gold standard answer has been available on a result page (for questions whose answers were meant to be lists, such as stroke risk factors)’

3.	‘for questions whose goal has been to obtain general information about a topic and the page contained at least one correct fact pertinent to the health topic’

‘The total number of assessed hits containing the gold standard answer have been recorded and the fraction of the assessed hits containing the gold standard answer (out of no more than 30) has been calculated’

‘This figure has been used as an estimate of precision and has been used for the comparison of searches as well’

‘Queries that returned no result or results that contained no gold standard answer have been considered as failed to satisfy users' information needs, although it is possible that there can be true negatives and returning results that do not contain the right information may misinform users and cost time to process’

‘For the convenience of performance comparison of all queries, when no result has been returned, the top 30 precision has been assigned 0 in this study’

‘True negative rates have been examined for those queries that failed to return any result’

‘To determine true negative rates, the researchers have conducted numerous searches of MedlinePlus and Google and browsed the concept-specific content of MedlinePlus to search for the gold standard answer’

‘Substitutions that seemed to improve results versus those that did not, have been examined for reasons. In addition, the queries and search results have been examined for general qualitative trends’

\textbf{Results}

‘A total of 16 queries have been selected for substitution from an initial pool of 46 queries’

‘Then 18 replacement queries have been generated’

‘Each query (consumer or reformulation) has been used to conduct two Internet searches, for a total of 68 searches: 34 in MedlinePlus and 34 in Google’

‘Overall, 926 individual search result pages have been examined’

‘Out of the 68 searches, 23 have not been able to yield any results. Nine of these empty searches have been generated using the consumers' original queries and 14 have been based on reformulated queries: 19 have been resulted in MedlinePlus and 4 have been resulted in Google’

‘In other words, it has shown that, using original queries in Google has resulted in the least number of searches (only 1) with no returns’

‘However, this study also has shown that, MedlinePlus contains the gold standard answer for 15 of the 19 failed queries (79%), and Google contains the answer for all 4 of the searches that had no returns (100%). Therefore, this means that among the queries that did not return any result, the true negative rate has been 21% for MedlinePlus and 0% for Google’

‘The success of a search has been measured using the percentage of the first 30 resulting pages that contained the gold standard answer (top 30 precision). For example, 10 of the first hits of the MedlinePlus search for "stroke" has contained enough of the established stroke risk factors to qualify as gold standard. There, this search has had a precision of 33% (10/30)’

‘However, the Google search for "thyroid antibody studies" has yielded only 11 hits. Out of these 11, 3 have contained the requested information regarding the anti-TPO lab test. This search therefore has had a precision of 27% (3/11)’

‘For this study, when a query generated no result, its top 30 precision has been set to 0’

‘According to the results, the mean precision for all searches in MedlinePlus has been 0.23 (SD = 0.37) and the mean precision for searches in Google has been 0.21 (SD = 0.26)’

‘According to the precision obtained, 15 of the 36 searches using reformulated queries have yielded better returns than their associated original queries (5 in MedlinePlus and 10 in Google). Seven of the searches using reformulated queries have shown a worse performance when compared to their original substitution as indicated by precision values (4 MedlinePlus and 3 Google)’

‘The performance of the remaining 14 searches have been identified as unchanged by reformulation’

‘Three reasons have been identified for changes in retrieval performance (precision)’

1.	‘Several consumer searches using ambiguous lay terms can be improved when reformulated with professional terminology. This trend has been noted both in MedlinePlus and in Google’

2.	‘Searches based on queries utilizing acronyms have been improved in the Google scope when they have been expanded to full phrases’

3.	‘Certain queries containing professional terms that have been arcane or contextually ill-fitted to the users' original search goals has performed worse than the original queries’

\textbf{Discussion}

\textbf{Significance and Implication}

‘Usually query reformulation has helped to improve query performance by reducing ambiguity and increasing distinguishing power, but sometimes reduced query performance when the professional terms have been identified as arcane or ill-fitted.’

‘It has been identified that approximately 35% of the consumer queries that have been collected did not use UMLS-preferred names for concepts and therefore, has been suitable for reformulation. Hence, this represents a substantial portion of the original sample queries that could potentially be affected by the reformulation approach’

\textbf{Advantages of query reformulation}

‘In 15 out of 36 instances, the replacement queries have yielded better results than the originals’

‘Even though many medical sites do employ lay terms, professional terms tend to have better distinguishing power in locating medical contents’

‘Acronyms or abbreviations are likely to introduce ambiguity to queries and thus are able benefit from reformulation. For instance, searches composed of acronyms or initialisms (eg, "SSRI" for selective serotonin reuptake inhibitor and "HRT" for hormone replacement therapy) or abbreviations (eg, "abs" for antibodies) has fared better when reformulated with the full phrase’. ‘However, this trend has only been noted in the broader Google domain’. ‘This has been reasonable because there is a much greater chance of the existence of a non-medical meaning of these short terms in the broader scope of Google than in the exclusively medical scope of MedlinePlus’

\textbf{Disadvantages of query reformulation}

‘In 7 out of 36 instances, search performance has been identified as worse when the original consumer queries have been replaced with alternate phrases’

‘Four of these have been conducted in the MedlinePlus domain, and three have been in Google’

‘According to these results, an automated query-replacement process has the potential for flawed substitutions. Therefore, in any case if a consumer has been presented with a search term that produces worse results than his or her original query, this has the ability to lead the consumer to great frustration’
 
\textbf{Content scope and quality}

‘According to this study, the role of query reformulation appears to be more significant for a large content scope than for a health-care specific site simply because there is more room for ambiguity when the scope is extremely large’

‘When considering a single site with medical information, because a single site contains a small fraction of the information available on the general Internet, the chance of a finding the desired information is identified to be diminished’

‘The true negative rate for MedlinePlus (21%) has been considerably higher than that for Google (0%), further emphasizing that medical sites, though providing a more consistent quality of information, has not been able to contain the answer for a percentage of consumers' queries’

‘Therefore, resolving the trade-off between quality and breadth of information is identified as a remaining major challenge to successful consumer information retrieval’

\textbf{Advantages to Queries With Known Goals}

‘The information needs of the consumer are explicit because they have been obtained from direct interviewing’

‘The information gained by interviewing participants have been really important to understand their actual information needs because, although many of the users had very precise goals, the queries they formulated have contained little more than the name of the disease or condition in question’

‘These observations also have provided further evidence to the reported observation that consumers are usually producing short, imprecise queries’

\textbf{Limitations}

‘The relatively small number of available sample queries with known goals has limited the analysis to qualitative review instead of a statistically significant quantitative measure of search precision’

‘The numbers of queries available for reformulation have been further limited by the requirement that the original term not be a Metathesaurus preferred concept name’

‘The gold standard answer has also presented another limitation. Because each site presents information differently, it is not possible to apply identical standards from site to site’

‘Using quotation marks for queries has been a major factor contributing to the number of searches that did not return results. Researchers have chosen to utilize quoted query phrases so that they could assess the impact of the phrase as a whole rather than the individual words, which each play a separate part in the search when not contained by quotation marks’

‘The method used to search MedlinePlus has had one limitation, because this site has been searched using Google instead of the search engine included with the site. Therefore, the search results have not been the same when the MedlinePlus search mechanism has been used instead of Google’

\textbf{Conclusion}

‘The study has been able to show some qualitative evidence that reformulating queries with professional terminology may be a promising strategy to improve consumer health-information searches’
 
‘A trend towards increased precision has been seen when providing substitutions for lay terms, abbreviations, and acronyms. This improvement has been noted both in searches conducted in the narrower scope of a consumer health site and in searches of a much broader portion of the Internet using the popular search engine Google’

‘However, automated reformulation also has had the ability to worsen search performance when the terminology has been ill-fitted or arcane’


\end{document}